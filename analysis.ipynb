{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not for commercial use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import sleep\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    balanced_accuracy_score,\n",
    "    confusion_matrix,\n",
    "    jaccard_score,\n",
    "    cohen_kappa_score\n",
    ")\n",
    "from lifelines.utils import concordance_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rater_files = {\n",
    "    \"gprater1\": {\n",
    "        \"colorectal\": \"gprater1/colorectal_gprater1.csv\",\n",
    "        \"dvt\": \"gprater1/dvt_gprater1.csv\",\n",
    "        \"pad\": \"gprater1/pad_gprater1.csv\",\n",
    "        \"pet\": \"gprater1/pet_gprater1.csv\",\n",
    "        \"spect\": \"gprater1/spect_gprater1.csv\",\n",
    "        \"stent\": \"gprater1/stent_gprater1.csv\",\n",
    "    },\n",
    "    \"gprater2\": {\n",
    "        \"colorectal\": \"gprater2/colorectal_gprater2.csv\",\n",
    "        \"dvt\": \"gprater2/dvt_gprater2.csv\",\n",
    "        \"pad\": \"gprater2/pad_gprater2.csv\",\n",
    "        \"pet\": \"gprater2/pet_gprater2.csv\",\n",
    "        \"spect\": \"gprater2/spect_gprater2.csv\",\n",
    "        \"stent\": \"gprater2/stent_gprater2.csv\",\n",
    "    },\n",
    "    \"gprater3\": {\n",
    "        \"colorectal\": \"gprater3/colorectal_gprater3.csv\",\n",
    "        \"dvt\": \"gprater3/dvt_gprater3.csv\",\n",
    "        \"pad\": \"gprater3/PAD_gprater3.csv\",\n",
    "        \"pet\": \"gprater3/PET_gprater3.csv\",\n",
    "        \"spect\": \"gprater3/SPECT_gprater3.csv\",\n",
    "        \"stent\": \"gprater3/STENT_gprater3.csv\",\n",
    "    },\n",
    "    \"erater1\": {\n",
    "        \"colorectal\": \"erater1/erater1_colorectal.csv\",\n",
    "        \"dvt\": \"erater1/erater1_dvt.csv\",\n",
    "        \"pad\": \"erater1/erater1_pad.csv\",\n",
    "        \"pet\": \"erater1/erater1_pet.csv\",\n",
    "        \"spect\": \"erater1/erater1_spect.csv\",\n",
    "        \"stent\": \"erater1/erater1_stent.csv\",\n",
    "    },\n",
    "    \"erater2\": {\n",
    "        \"colorectal\": \"erater2/erater2_colorectal.csv\",\n",
    "        \"dvt\": \"erater2/erater2_dvt.csv\",\n",
    "        \"pad\": \"erater2/erater2_pad.csv\",\n",
    "        \"pet\": \"erater2/erater2_pet.csv\",\n",
    "        \"spect\": \"erater2/erater2_spect.csv\",\n",
    "        \"stent\": \"erater2/erater2_stent.csv\",\n",
    "    },\n",
    "    \"erater3\": {\n",
    "        \"colorectal\": \"erater3/colorectal.csv\",\n",
    "        \"dvt\": \"erater3/dvt.csv\",\n",
    "        \"pad\": \"erater3/pad.csv\",\n",
    "        \"pet\": \"erater3/pet.csv\",\n",
    "        \"spect\": \"erater3/spect.csv\",\n",
    "        \"stent\": \"erater3/stent.csv\",\n",
    "    },\n",
    "    \"gpt\": {\n",
    "        \"colorectal\": \"GPT3.5/gpt_colorectal.csv\",\n",
    "        \"dvt\": \"GPT3.5/gpt_dvt.csv\",\n",
    "        \"pad\": \"GPT3.5/gpt_pad.csv\",\n",
    "        \"pet\": \"GPT3.5/gpt_pet.csv\",\n",
    "        \"spect\": \"GPT3.5/gpt_spect.csv\",\n",
    "        \"stent\": \"GPT3.5/gpt_stent.csv\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raters = list(rater_files.keys())\n",
    "articles = list(rater_files[raters[0]].keys())\n",
    "\n",
    "human_raters = [rater for rater in raters if \"gpt\" not in rater]\n",
    "\n",
    "gpt_raters = [rater for rater in raters if rater not in human_raters]\n",
    "\n",
    "gp_raters = [\"gprater2\", \"gprater1\", \"gprater3\"]\n",
    "expert_raters = [\"erater1\", \"erater2\", \"erater3\"]\n",
    "\n",
    "print(\"all raters:\", raters)\n",
    "print(\"human raters:\", human_raters)\n",
    "print(\"gpt raters:\", gpt_raters)\n",
    "print(\"gp raters:\", gp_raters)\n",
    "print(\"expert raters:\", expert_raters)\n",
    "print(\"article types:\", articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_cutoff_dict = {\n",
    "    \"gpt\": 2,\n",
    "}\n",
    "gp_range = (3, 5)\n",
    "\n",
    "name_dict = {\n",
    "    \"gpt\": \"ChatGPT\",\n",
    "    \"gprater1\": \"GP 1\",\n",
    "    \"gprater2\": \"GP 2\",\n",
    "    \"gprater3\": \"GP 3\",\n",
    "    \"erater1\": \"Expert 1\",\n",
    "    \"erater2\": \"Expert 2\",\n",
    "    \"erater3\": \"Expert 3\",\n",
    "    \"vote\": \"Voting Consensus\",\n",
    "    \"spec_con\": \"Specific Consensus\",\n",
    "    \"sens_con\": \"Sensitive Consensus\",\n",
    "    \"vote_gp\": \"Voting Consensus (GPs)\",\n",
    "    \"spec_con_gp\": \"Specific Consensus (GPs)\",\n",
    "    \"sens_con_gp\": \"Sensitive Consensus (GPs)\",\n",
    "}\n",
    "\n",
    "consensus_types = [\"vote\", \"spec_con\", \"sens_con\"]\n",
    "consensus_types_gp = [\"vote_gp\", \"spec_con_gp\", \"sens_con_gp\"]\n",
    "consensus_raters = expert_raters.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inclusion_field = \"inclusion\"\n",
    "inclusion_original_field = \"inclusion_original\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict = {\n",
    "    \"sen\": lambda _, _2, fn, tp, _3: tp / (tp + fn),\n",
    "    \"spec\": lambda tn, fp, _, _2, _3: tn / (tn + fp),\n",
    "    \"ppv\": lambda _, fp, _2, tp, _3: tp / (tp + fp),\n",
    "    \"npv\": lambda tn, _, fn, _2, _3: tn / (tn + fn),\n",
    "    \"plr\": lambda tn, fp, fn, tp, epsilon: (tp / (tp + fn))\n",
    "    / (1 - (tn / (tn + fp)) - epsilon),\n",
    "    \"nlr\": lambda tn, fp, fn, tp, epsilon: (\n",
    "        (1 - (tp / (tp + fn))) / ((tn / (tn + fp)) + epsilon)\n",
    "    ),\n",
    "}\n",
    "metrics = list(metric_dict.keys())\n",
    "metric_name_dict = {\n",
    "    \"sen\": \"Sensitivity\",\n",
    "    \"spec\": \"Specificity\",\n",
    "    \"ppv\": \"Positive Predictive Value\",\n",
    "    \"npv\": \"Negative Predictive Value\",\n",
    "    \"plr\": \"Positive Likelihood Ratio\",\n",
    "    \"nlr\": \"Negative Likelihood Ratio\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe for each csv file using a dictionary comprehension\n",
    "dfs = {\n",
    "    rater: {\n",
    "        article: pd.read_csv(rater_files[rater][article])\n",
    "        for article in articles\n",
    "        if article in rater_files[rater]\n",
    "    }\n",
    "    for rater in raters\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cutoff(\n",
    "    cutoff: dict[str, int],\n",
    "    gpts: list[str] = [\"gpt\"],\n",
    "    articles: list[str] = articles,\n",
    "    inclusion_field: str = inclusion_field,\n",
    "    inclusion_original_field: str = inclusion_original_field,\n",
    "):\n",
    "    for rater in gpts:\n",
    "        for article in articles:\n",
    "            if article in rater_files[rater]:\n",
    "                dfs[rater][article][inclusion_field] = (\n",
    "                    dfs[rater][article][inclusion_original_field] > cutoff[rater]\n",
    "                ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in gpt inclusion column, use the cutoff value to discretize the values\n",
    "# before applying the cutoff, save the original values in a new column\n",
    "for rater in gpt_raters:\n",
    "    for article in articles:\n",
    "        if article in rater_files[rater]:\n",
    "            dfs[rater][article][inclusion_original_field] = dfs[rater][article][\n",
    "                inclusion_field\n",
    "            ]\n",
    "apply_cutoff(gpt_cutoff_dict, gpt_raters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in other raters, map include to 1 and exclude to 0 in the inclusion column\n",
    "for rater in human_raters:\n",
    "    for article in articles:\n",
    "        if article in rater_files[rater]:\n",
    "            dfs[rater][article][inclusion_field] = dfs[rater][article][\n",
    "                inclusion_field\n",
    "            ].map({\"include\": 1, \"exclude\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if inclusion column has nan values\n",
    "for rater in raters:\n",
    "    for article in articles:\n",
    "        print(\n",
    "            rater,\n",
    "            article,\n",
    "            dfs[rater][article][inclusion_field].isnull().values.any()\n",
    "            if article in dfs[rater]\n",
    "            else \"not found\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of rows in each dataframe\n",
    "for rater in raters:\n",
    "    for article in articles:\n",
    "        print(\n",
    "            rater,\n",
    "            article,\n",
    "            dfs[rater][article].shape[0] if article in dfs[rater] else \"not found\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dfs by title so we can compare them\n",
    "for rater in raters:\n",
    "    for article in articles:\n",
    "        if article in dfs[rater]:\n",
    "            dfs[rater][article] = dfs[rater][article].sort_values(by=[\"Title\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the indices\n",
    "for rater in raters:\n",
    "    for article in articles:\n",
    "        if article in dfs[rater]:\n",
    "            dfs[rater][article] = dfs[rater][article].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare Titles from each rater\n",
    "# to make sure they are the same\n",
    "for article in articles:\n",
    "    print(\"\\n\", article)\n",
    "    for rater in raters:\n",
    "        if article in dfs[rater]:\n",
    "            print(\n",
    "                rater,\n",
    "                \"OK\"\n",
    "                if dfs[rater][article][\"Title\"].equals(dfs[raters[0]][article][\"Title\"])\n",
    "                else \"Not Identical\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# concatenate the dfs for each rater\n",
    "# so we can compare them\n",
    "dfs_concat = {\n",
    "    rater: pd.concat(\n",
    "        [dfs[rater][article] for article in articles if article in dfs[rater]]\n",
    "    )\n",
    "    for rater in raters\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# use the expert raters to reach a sensitive consensus\n",
    "# but keep the column names the same\n",
    "dfs_concat[\"sens_con\"] = dfs_concat[consensus_raters[0]].copy()\n",
    "for rater in consensus_raters:\n",
    "    if rater != consensus_raters[0]:\n",
    "        dfs_concat[\"sens_con\"][inclusion_field] += dfs_concat[rater][inclusion_field]\n",
    "# then divide by the number of raters to get the average\n",
    "dfs_concat[\"sens_con\"][inclusion_field] = (\n",
    "    dfs_concat[\"sens_con\"][inclusion_field] > 0\n",
    ")  # we want to include if any rater included\n",
    "dfs_concat[\"sens_con\"][inclusion_field] = dfs_concat[\"sens_con\"][\n",
    "    inclusion_field\n",
    "].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# use the gp raters to reach a sensitive consensus\n",
    "# but keep the column names the same\n",
    "dfs_concat[\"sens_con_gp\"] = dfs_concat[gp_raters[0]].copy()\n",
    "for rater in gp_raters:\n",
    "    if rater != gp_raters[0]:\n",
    "        dfs_concat[\"sens_con_gp\"][inclusion_field] += dfs_concat[rater][inclusion_field]\n",
    "dfs_concat[\"sens_con_gp\"][inclusion_field] = (\n",
    "    dfs_concat[\"sens_con_gp\"][inclusion_field] > 0\n",
    ")  # we want to include if any rater included\n",
    "dfs_concat[\"sens_con_gp\"][inclusion_field] = dfs_concat[\"sens_con_gp\"][\n",
    "    inclusion_field\n",
    "].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# use the expert raters to reach a specific_spec_con\n",
    "# but keep the column names the same\n",
    "dfs_concat[\"spec_con\"] = dfs_concat[consensus_raters[0]].copy()\n",
    "for rater in consensus_raters:\n",
    "    if rater != consensus_raters[0]:\n",
    "        dfs_concat[\"spec_con\"][inclusion_field] += dfs_concat[rater][inclusion_field]\n",
    "# then divide by the number of raters to get the average\n",
    "# we want to include if only rated positive by all raters (specific consensus)\n",
    "dfs_concat[\"spec_con\"][inclusion_field] = dfs_concat[\"spec_con\"][\n",
    "    inclusion_field\n",
    "] == len(consensus_raters)\n",
    "dfs_concat[\"spec_con\"][inclusion_field] = dfs_concat[\"spec_con\"][\n",
    "    inclusion_field\n",
    "].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# use the gp raters to reach a specific_spec_con\n",
    "# but keep the column names the same\n",
    "dfs_concat[\"spec_con_gp\"] = dfs_concat[gp_raters[0]].copy()\n",
    "for rater in gp_raters:\n",
    "    if rater != gp_raters[0]:\n",
    "        dfs_concat[\"spec_con_gp\"][inclusion_field] += dfs_concat[rater][inclusion_field]\n",
    "# then divide by the number of raters to get the average\n",
    "# we want to include if only rated positive by all raters (specific consensus)\n",
    "dfs_concat[\"spec_con_gp\"][inclusion_field] = dfs_concat[\"spec_con_gp\"][\n",
    "    inclusion_field\n",
    "] == len(gp_raters)\n",
    "dfs_concat[\"spec_con_gp\"][inclusion_field] = dfs_concat[\"spec_con_gp\"][\n",
    "    inclusion_field\n",
    "].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# use the expert raters to reach a consensus\n",
    "# but keep the column names the same\n",
    "dfs_concat[\"vote\"] = dfs_concat[consensus_raters[0]].copy()\n",
    "for rater in consensus_raters:\n",
    "    if rater != consensus_raters[0]:\n",
    "        dfs_concat[\"vote\"][inclusion_field] += dfs_concat[rater][inclusion_field]\n",
    "# then divide by the number of raters to get the average\n",
    "# we want to include if the average is greater than 0.5\n",
    "dfs_concat[\"vote\"][inclusion_field] /= len(consensus_raters)\n",
    "# round the values to the nearest integer\n",
    "dfs_concat[\"vote\"][inclusion_field] = dfs_concat[\"vote\"][inclusion_field] > 0.5\n",
    "dfs_concat[\"vote\"][inclusion_field] = dfs_concat[\"vote\"][inclusion_field].astype(\n",
    "    int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# use the gp raters to reach a consensus\n",
    "# but keep the column names the same\n",
    "dfs_concat[\"vote_gp\"] = dfs_concat[gp_raters[0]].copy()\n",
    "for rater in gp_raters:\n",
    "    if rater != gp_raters[0]:\n",
    "        dfs_concat[\"vote_gp\"][inclusion_field] += dfs_concat[rater][inclusion_field]\n",
    "# then divide by the number of raters to get the average\n",
    "# we want to include if the average is greater than 0.5\n",
    "dfs_concat[\"vote_gp\"][inclusion_field] /= len(gp_raters)\n",
    "# round the values to the nearest integer\n",
    "dfs_concat[\"vote_gp\"][inclusion_field] = dfs_concat[\"vote_gp\"][inclusion_field] > 0.5\n",
    "dfs_concat[\"vote_gp\"][inclusion_field] = dfs_concat[\"vote_gp\"][inclusion_field].astype(\n",
    "    int\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def get_kappas(raters: list[str], dfs_concat: dict[str, pd.DataFrame] = dfs_concat):\n",
    "    \"\"\"get the kappas between raters\n",
    "\n",
    "    Args:\n",
    "        raters (list[str]): raters to get kappas for\n",
    "        dfs_concat (dict[str, pd.DataFrame], optional): dataframes dict to get kappas from. Defaults to dfs_concat.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: kappas between raters of shape (len(raters), len(raters))\n",
    "    \"\"\"\n",
    "    rater_kappas = np.zeros((len(raters), len(raters)))\n",
    "    mask = np.tri(rater_kappas.shape[-1], k=-1)\n",
    "    for rater1 in raters:\n",
    "        for rater2 in raters:\n",
    "            if len(dfs_concat[rater1]) == len(dfs_concat[rater2]):\n",
    "                rater_kappas[\n",
    "                    raters.index(rater1), raters.index(rater2)\n",
    "                ] = cohen_kappa_score(\n",
    "                    dfs_concat[rater1][inclusion_field],\n",
    "                    dfs_concat[rater2][inclusion_field],\n",
    "                )\n",
    "    rater_kappas *= mask\n",
    "    return rater_kappas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kappas_article(\n",
    "    raters: list[str], articles: list[str], dfs: dict[str, dict[str, pd.DataFrame]]\n",
    "):\n",
    "    \"\"\"get the kappas between raters for each article\n",
    "\n",
    "    Args:\n",
    "        raters (list[str]): raters to get kappas for\n",
    "        articles (list[str]): articles to get kappas for\n",
    "        dfs (dict[str, dict[str, pd.DataFrame]]): dataframes dict to get kappas from\n",
    "\n",
    "    Returns:\n",
    "          numpy.ndarray: kappas between raters of shape (len(articles), len(raters), len(raters))\n",
    "    \"\"\"\n",
    "    # calculate the kohen kappa score between raters where possible\n",
    "    article_rater_kappas = np.zeros((len(articles), len(raters), len(raters)))\n",
    "    for article in articles:\n",
    "        for rater1 in raters:\n",
    "            for rater2 in raters:\n",
    "                if article in dfs[rater1] and article in dfs[rater2]:\n",
    "                    kappa = cohen_kappa_score(\n",
    "                        dfs[rater1][article][inclusion_field],\n",
    "                        dfs[rater2][article][inclusion_field],\n",
    "                    )\n",
    "                    article_rater_kappas[articles.index(article)][raters.index(rater1)][\n",
    "                        raters.index(rater2)\n",
    "                    ] = kappa\n",
    "    # to remove the lower triangle we multiply the matrix by a mask\n",
    "    mask = np.tri(article_rater_kappas.shape[-1], k=-1)\n",
    "    article_rater_kappas = article_rater_kappas * mask\n",
    "    return article_rater_kappas  # shape: (num_articles, num_raters, num_raters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kappas(\n",
    "    raters: list[str],\n",
    "    dfs_concat: dict[str, pd.DataFrame] = dfs_concat,\n",
    "    name_dict: dict[str, str] = name_dict,\n",
    "    figsize: tuple[float, float] = (10, 10),\n",
    "    save_path: str = None,\n",
    "):\n",
    "    \"\"\"plot the kappas between raters\n",
    "\n",
    "    Args:\n",
    "        raters (list[str]): raters to plot kappas for\n",
    "        dfs_concat (dict[str, pd.DataFrame], optional): dataframes dict to get kappas from. Defaults to dfs_concat.\n",
    "        name_dict (dict[str, str], optional): dict to map raters to names. Defaults to name_dict.\n",
    "        figsize (tuple[float, float], optional): figure size. Defaults to (10, 10).\n",
    "        save_path (str, optional): path to save figure to. Defaults to None.\n",
    "    \"\"\"\n",
    "    kappas = get_kappas(raters, dfs_concat)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    rater_names = list(map(lambda x: name_dict[x], raters))\n",
    "    sns.heatmap(\n",
    "        kappas,\n",
    "        annot=True,\n",
    "        xticklabels=rater_names,\n",
    "        yticklabels=rater_names,\n",
    "        cmap=\"Blues\",\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        cbar=False,\n",
    "        square=True,\n",
    "        ax=ax,\n",
    "    )\n",
    "    ax.set_title(\"Kappa scores for raters\")\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kappas_article(\n",
    "    articles: list[str],\n",
    "    raters: list[str],\n",
    "    dfs: dict[str, dict[str, pd.DataFrame]] = dfs,\n",
    "    name_dict: dict[str, str] = name_dict,\n",
    "    n_cols: int = 2,\n",
    "    size: float = 5,\n",
    "    save_path: str = None,\n",
    "):\n",
    "    \"\"\"Plot the kappas for each article\n",
    "\n",
    "    Args:\n",
    "        articles (list[str]): the articles to plot\n",
    "        raters (list[str]): the raters to plot\n",
    "        dfs (dict[str, dict[str, pd.DataFrame]], optional): dfs containing the data. Defaults to dfs.\n",
    "        name_dict (dict[str, str], optional): dictionary mapping rater names to the names to be displayed on the plot. Defaults to name_dict.\n",
    "        n_cols (int, optional): number of columns in the plot. Defaults to 2.\n",
    "        size (float, optional): size of each plot. Defaults to 5.\n",
    "    \"\"\"\n",
    "    kappas_matrix = get_kappas_article(raters, articles, dfs)\n",
    "    nrows = len(articles) // n_cols + len(articles) % n_cols\n",
    "    _, axs = plt.subplots(\n",
    "        nrows=nrows,\n",
    "        ncols=n_cols,\n",
    "        figsize=(size * n_cols, size * nrows),\n",
    "    )\n",
    "    rater_names = [name_dict[rater] for rater in raters]\n",
    "    for i, article in enumerate(articles):\n",
    "        sns.heatmap(\n",
    "            kappas_matrix[articles.index(article)],\n",
    "            annot=True,\n",
    "            xticklabels=rater_names,\n",
    "            yticklabels=rater_names,\n",
    "            cmap=\"Blues\",\n",
    "            square=True,\n",
    "            cbar=False,\n",
    "            ax=axs[i // n_cols, i % n_cols],\n",
    "        )\n",
    "        axs[i // n_cols, i % n_cols].set_title(f\"{article.upper()} Kappa\")\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_kappa_ci(\n",
    "    raters: list[str],\n",
    "    name_dict: dict[str, str],\n",
    "    dfs_concat: dict[str, pd.DataFrame] = dfs_concat,\n",
    "    n_bootstraps: int = 1000,\n",
    "    ci_level: float = 0.95,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"print the confidence interval for the kappa scores between raters\n",
    "\n",
    "    Args:\n",
    "        raters (list[str]): raters to get kappas for\n",
    "        name_dict (dict[str, str]): dict to map raters to names\n",
    "        dfs_concat (dict[str, pd.DataFrame], optional): dataframes dict to get kappas from. Defaults to dfs_concat.\n",
    "        n_bootstraps (int, optional): number of bootstraps to use. Defaults to 1000.\n",
    "        ci_level (float, optional): confidence interval level. Defaults to 0.95.\n",
    "        seed (int, optional): seed for the random number generator. Defaults to 42.\n",
    "    \"\"\"\n",
    "    for i, rater1 in enumerate(raters):\n",
    "        for j, rater2 in enumerate(raters):\n",
    "            if j <= i:\n",
    "                continue\n",
    "            print(f\"{name_dict[rater1]} vs {name_dict[rater2]}\")\n",
    "            observed_kappa = cohen_kappa_score(\n",
    "                dfs_concat[rater1][inclusion_field], dfs_concat[rater2][inclusion_field]\n",
    "            )\n",
    "            rng = np.random.default_rng(seed)\n",
    "            kappas = []\n",
    "            for _ in range(n_bootstraps):\n",
    "                bs_indices = rng.choice(\n",
    "                    len(dfs_concat[rater1]), len(dfs_concat[rater1]), replace=True\n",
    "                )\n",
    "                kappa = cohen_kappa_score(\n",
    "                    dfs_concat[rater1][inclusion_field].iloc[bs_indices],\n",
    "                    dfs_concat[rater2][inclusion_field].iloc[bs_indices],\n",
    "                )\n",
    "                kappas.append(kappa)\n",
    "            kappas = np.array(kappas)\n",
    "            lower_ci = np.quantile(kappas, (1 - ci_level) / 2)\n",
    "            upper_ci = np.quantile(kappas, 1 - (1 - ci_level) / 2)\n",
    "            print(\n",
    "                f\"Kappa [{ci_level*100:.0f}% CI]: {observed_kappa:.2f} [{lower_ci:.2f}, {upper_ci:.2f}]\".rjust(\n",
    "                    20\n",
    "                )\n",
    "            )\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(\n",
    "    gold_standards: list[str],\n",
    "    comparators: list[str],\n",
    "    dfs_concat: dict[str, pd.DataFrame] = dfs_concat,\n",
    "    name_dict: dict[str, str] = name_dict,\n",
    "    size: float = 3,\n",
    "    save_path: str = None,\n",
    "):\n",
    "    \"\"\"Plot the ROC curves for the comparators against the gold standards\n",
    "\n",
    "    Args:\n",
    "        gold_standards (list[str]): gold standards to evaluate against\n",
    "        comparators (list[str]): comparators to evaluate\n",
    "        dfs_concat (dict[str, pd.DataFrame], optional): dfs to get data from. Defaults to dfs_concat.\n",
    "        name_dict (dict[str, str], optional): dict to map names to display names. Defaults to name_dict.\n",
    "        size (float, optional): size of each plot. Defaults to 3.\n",
    "        save_path (str, optional): path to save the plot to. Defaults to None.\n",
    "    \"\"\"\n",
    "    _, axs = plt.subplots(\n",
    "        nrows=len(gold_standards),\n",
    "        ncols=len(comparators),\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "        figsize=(len(gold_standards) * size, len(comparators) * (size) * 1.2),\n",
    "    )\n",
    "    for j, gold_standard in enumerate(gold_standards):\n",
    "        for i, comparator in enumerate(comparators):\n",
    "            fpr, tpr, thresholds = roc_curve(\n",
    "                dfs_concat[gold_standard][inclusion_field],\n",
    "                dfs_concat[comparator][inclusion_original_field],\n",
    "            )\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            youden = np.argmax(tpr - fpr)\n",
    "            axs[j, i].plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "            axs[j, i].plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "            axs[j, i].plot(\n",
    "                fpr[youden],\n",
    "                tpr[youden],\n",
    "                marker=\"o\",\n",
    "                markersize=10,\n",
    "                label=f\"Youden threshold = {thresholds[youden]:.0f}\",\n",
    "            )\n",
    "            axs[j, i].set_xlabel(\"False Positive Rate\")\n",
    "            axs[j, i].set_ylabel(\"True Positive Rate\")\n",
    "            axs[j, i].set_title(\n",
    "                f\"{name_dict[comparator]} vs {name_dict[gold_standard]}\"\n",
    "            )\n",
    "            axs[j, i].legend(loc=\"lower right\")\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    else: \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_ci(\n",
    "    gold_standards: list[str],\n",
    "    comparators: list[str],\n",
    "    dfs_concat: dict[str, pd.DataFrame] = dfs_concat,\n",
    "    name_dict: dict[str, str] = name_dict,\n",
    "    size: float = 5,\n",
    "    bootstrap_samples: int = 1000,\n",
    "    ci_level: float = 0.95,\n",
    "    save_path: str = None,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"Plot the ROC curves for the comparators against the gold standards with confidence intervals\n",
    "\n",
    "    Args:\n",
    "        gold_standards (list[str]): gold standards to evaluate against\n",
    "        comparators (list[str]): comparators to evaluate\n",
    "        dfs_concat (dict[str, pd.DataFrame], optional): dict of dfs to get data from. Defaults to dfs_concat.\n",
    "        name_dict (dict[str, str], optional): dict to map names to display names. Defaults to name_dict.\n",
    "        size (float, optional): size of each plot. Defaults to 5.\n",
    "        bootstrap_samples (int, optional): number of bootstrap samples to use. Defaults to 1000.\n",
    "        ci_level (float, optional): confidence interval level. Defaults to 0.95.\n",
    "        save_path (str, optional): path to save the plot to. Defaults to None.\n",
    "        seed (int, optional): random seed. Defaults to 42.\n",
    "    \"\"\"\n",
    "    _, axs = plt.subplots(\n",
    "        nrows=len(gold_standards),\n",
    "        ncols=len(comparators),\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "        figsize=(len(gold_standards) * size, len(comparators) * (size) * 1.5),\n",
    "    )\n",
    "    for j, gold_standard in enumerate(gold_standards):\n",
    "        for i, comparator in enumerate(comparators):\n",
    "            # calculate original ROC curve\n",
    "            fpr, tpr, thresholds = roc_curve(\n",
    "                dfs_concat[gold_standard][inclusion_field],\n",
    "                dfs_concat[comparator][inclusion_original_field],\n",
    "            )\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            youden = np.argmax(tpr - fpr)\n",
    "\n",
    "            # calculate ROC curves for bootstrapped samples\n",
    "            bootstrapped_tprs = []\n",
    "            aucs = []\n",
    "            rng = np.random.default_rng(seed)\n",
    "            for _ in range(bootstrap_samples):\n",
    "                # resample with replacement\n",
    "                bootstrap_indices = rng.choice(\n",
    "                    np.arange(len(dfs_concat[gold_standard][inclusion_field])),\n",
    "                    size=len(dfs_concat[gold_standard][inclusion_field]),\n",
    "                )\n",
    "                bootstrapped_fpr, bootstrapped_tpr, _ = roc_curve(\n",
    "                    dfs_concat[gold_standard][inclusion_field].iloc[bootstrap_indices],\n",
    "                    dfs_concat[comparator][inclusion_original_field].iloc[\n",
    "                        bootstrap_indices\n",
    "                    ],\n",
    "                )\n",
    "                aucs.append(auc(bootstrapped_fpr, bootstrapped_tpr))\n",
    "                bootstrapped_tprs.append(\n",
    "                    np.interp(\n",
    "                        np.linspace(0, 1, 100), bootstrapped_fpr, bootstrapped_tpr\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # calculate lower and upper confidence intervals\n",
    "            bootstrapped_tprs = np.array(bootstrapped_tprs)\n",
    "            tprs_lower = np.percentile(\n",
    "                bootstrapped_tprs, ((1.0 - ci_level) / 2) * 100, axis=0\n",
    "            )\n",
    "            tprs_upper = np.percentile(\n",
    "                bootstrapped_tprs, (1.0 - ((1.0 - ci_level) / 2)) * 100, axis=0\n",
    "            )\n",
    "            aucs = np.array(aucs)\n",
    "            aucs_lower = np.percentile(aucs, ((1.0 - ci_level) / 2) * 100, axis=0)\n",
    "            aucs_upper = np.percentile(\n",
    "                aucs, (1.0 - ((1.0 - ci_level) / 2)) * 100, axis=0\n",
    "            )\n",
    "\n",
    "            if len(gold_standards) == len(comparators) == 1:\n",
    "                axs.plot(\n",
    "                    fpr,\n",
    "                    tpr,\n",
    "                    label=f\"ROC (AUC = {roc_auc:.2f} [{aucs_lower:.2f},{aucs_upper:.2f}])\",\n",
    "                )\n",
    "                axs.fill_between(\n",
    "                    np.linspace(0, 1, 100),\n",
    "                    tprs_lower,\n",
    "                    tprs_upper,\n",
    "                    color=\"lightblue\",\n",
    "                    alpha=0.3,\n",
    "                    label=f\"{ci_level*100:.0f}% CI\",\n",
    "                )\n",
    "                axs.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "                axs.plot(\n",
    "                    fpr[youden],\n",
    "                    tpr[youden],\n",
    "                    marker=\"o\",\n",
    "                    markersize=10,\n",
    "                    label=f\"Youden threshold = {thresholds[youden]:.0f}\",\n",
    "                )\n",
    "\n",
    "                axs.set_xlabel(\"False Positive Rate\")\n",
    "                axs.set_ylabel(\"True Positive Rate\")\n",
    "                axs.set_title(f\"{name_dict[comparator]} vs {name_dict[gold_standard]}\")\n",
    "                axs.set_xlim([0, 1])\n",
    "                axs.set_ylim([0, 1])\n",
    "                # square axes\n",
    "                axs.set_aspect(\"equal\", adjustable=\"box\")\n",
    "                axs.legend(loc=\"lower right\")\n",
    "\n",
    "            elif len(gold_standards) == 1 or len(comparators) == 1:\n",
    "                axs[i + j].plot(\n",
    "                    fpr,\n",
    "                    tpr,\n",
    "                    label=f\"ROC (AUC = {roc_auc:.2f} [{aucs_lower:.2f},{aucs_upper:.2f}])\",\n",
    "                )\n",
    "                axs[i + j].fill_between(\n",
    "                    np.linspace(0, 1, 100),\n",
    "                    tprs_lower,\n",
    "                    tprs_upper,\n",
    "                    color=\"lightblue\",\n",
    "                    alpha=0.3,\n",
    "                    label=f\"{ci_level*100:.0f}% CI\",\n",
    "                )\n",
    "                axs[i + j].plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "                axs[i + j].plot(\n",
    "                    fpr[youden],\n",
    "                    tpr[youden],\n",
    "                    marker=\"o\",\n",
    "                    markersize=10,\n",
    "                    label=f\"Youden threshold = {thresholds[youden]:.0f}\",\n",
    "                )\n",
    "\n",
    "                axs[i + j].set_xlabel(\"False Positive Rate\")\n",
    "                axs[i + j].set_ylabel(\"True Positive Rate\")\n",
    "                axs[i + j].set_title(\n",
    "                    f\"{name_dict[comparator]} vs {name_dict[gold_standard]}\"\n",
    "                )\n",
    "                axs[i + j].set_xlim([0, 1])\n",
    "                axs[i + j].set_ylim([0, 1])\n",
    "                # square axes\n",
    "                axs[i + j].set_aspect(\"equal\", adjustable=\"box\")\n",
    "                axs[i + j].legend(loc=\"lower right\")\n",
    "\n",
    "            else:\n",
    "                axs[j, i].plot(\n",
    "                    fpr,\n",
    "                    tpr,\n",
    "                    label=f\"ROC (AUC = {roc_auc:.2f} [{aucs_lower:.2f},{aucs_upper:.2f}])\",\n",
    "                )\n",
    "                axs[j, i].fill_between(\n",
    "                    np.linspace(0, 1, 100),\n",
    "                    tprs_lower,\n",
    "                    tprs_upper,\n",
    "                    color=\"lightblue\",\n",
    "                    alpha=0.3,\n",
    "                    label=f\"{ci_level*100:.0f}% CI\",\n",
    "                )\n",
    "                axs[j, i].plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "                axs[j, i].plot(\n",
    "                    fpr[youden],\n",
    "                    tpr[youden],\n",
    "                    marker=\"o\",\n",
    "                    markersize=10,\n",
    "                    label=f\"Youden threshold = {thresholds[youden]:.0f}\",\n",
    "                )\n",
    "\n",
    "                axs[j, i].set_xlabel(\"False Positive Rate\")\n",
    "                axs[j, i].set_ylabel(\"True Positive Rate\")\n",
    "                axs[j, i].set_title(\n",
    "                    f\"{name_dict[comparator]} vs {name_dict[gold_standard]}\"\n",
    "                )\n",
    "                axs[j, i].set_xlim([0, 1])\n",
    "                axs[j, i].set_ylim([0, 1])\n",
    "                # square axes\n",
    "                axs[j, i].set_aspect(\"equal\", adjustable=\"box\")\n",
    "                axs[j, i].legend(loc=\"lower right\")\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion(\n",
    "    gold_standards: list[str],\n",
    "    comparators: list[str],\n",
    "    dfs_concat: dict[str, pd.DataFrame] = dfs_concat,\n",
    "    name_dict: dict[str, str] = name_dict,\n",
    "    size: float = 5,\n",
    "    save_path: str = None,\n",
    "):\n",
    "    \"\"\"plot confusion matrix for each gold standard and comparator\n",
    "\n",
    "    Args:\n",
    "        gold_standards (list[str]): gold standard names to evaluate against\n",
    "        comparators (list[str]): comparator names to evaluate\n",
    "        dfs_concat (dict[str, pd.DataFrame], optional): dictionary of dataframes. Defaults to dfs_concat.\n",
    "        name_dict (dict[str, str], optional): dictionary of names. Defaults to name_dict.\n",
    "        size (float, optional): size of each plot. Defaults to 5.\n",
    "        save_path (str, optional): path to save figure. Defaults to None.\n",
    "    \"\"\"\n",
    "    _, axs = plt.subplots(\n",
    "        len(comparators),\n",
    "        len(gold_standards),\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "        figsize=(len(comparators) * size, len(gold_standards) * size),\n",
    "    )\n",
    "\n",
    "    for i, comparator in enumerate(comparators):\n",
    "        print(name_dict[comparator])\n",
    "        for j, gold_standard in enumerate(gold_standards):\n",
    "            print(name_dict[gold_standard])\n",
    "            cm = confusion_matrix(\n",
    "                dfs_concat[gold_standard][inclusion_field],\n",
    "                dfs_concat[comparator][inclusion_field],\n",
    "            )\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            npv = tn / (tn + fn)\n",
    "            ppv = tp / (tp + fp)\n",
    "            sen = tp / (tp + fn)\n",
    "            spec = tn / (tn + fp)\n",
    "            plr = sen / (1 - spec)\n",
    "            nlr = (1 - sen) / spec\n",
    "            print(f\"tn: {tn}, fp: {fp}, fn: {fn}, tp: {tp}\")\n",
    "            print(\n",
    "                f\"NPV: {npv:.2f}, PPV: {ppv:.2f}, \\nSensitivity: {sen:.2f}, Specificity: {spec:.2f}\\nPLR: {plr:.2f}, NLR: {nlr:.2f}\"\n",
    "            )\n",
    "            # plot confusion matrix\n",
    "            # normalize the confusion matrix\n",
    "            cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "            if len(comparators) == len(gold_standards) == 1:\n",
    "                sns.heatmap(\n",
    "                    cm,\n",
    "                    annot=True,\n",
    "                    xticklabels=[\"Excluded\", \"Included\"],\n",
    "                    yticklabels=[\"Excluded\", \"Included\"],\n",
    "                    cmap=\"Blues\",\n",
    "                    vmin=0,\n",
    "                    vmax=1,\n",
    "                    square=True,\n",
    "                    cbar=False,\n",
    "                    ax=axs,\n",
    "                )\n",
    "                axs.set_xlabel(name_dict[comparator])\n",
    "                axs.set_ylabel(name_dict[gold_standard])\n",
    "                axs.set_title(f\"{name_dict[comparator]}\")\n",
    "            elif len(comparators) == 1 or len(gold_standards) == 1:\n",
    "                sns.heatmap(\n",
    "                    cm,\n",
    "                    annot=True,\n",
    "                    xticklabels=[\"Excluded\", \"Included\"],\n",
    "                    yticklabels=[\"Excluded\", \"Included\"],\n",
    "                    cmap=\"Blues\",\n",
    "                    vmin=0,\n",
    "                    vmax=1,\n",
    "                    square=True,\n",
    "                    cbar=False,\n",
    "                    ax=axs[i + j],\n",
    "                )\n",
    "                axs[i + j].set_xlabel(name_dict[comparator])\n",
    "                axs[i + j].set_ylabel(name_dict[gold_standard])\n",
    "                axs[i + j].set_title(f\"{name_dict[comparator]}\")\n",
    "            else:\n",
    "                sns.heatmap(\n",
    "                    cm,\n",
    "                    annot=True,\n",
    "                    xticklabels=[\"Excluded\", \"Included\"],\n",
    "                    yticklabels=[\"Excluded\", \"Included\"],\n",
    "                    cmap=\"Blues\",\n",
    "                    vmin=0,\n",
    "                    vmax=1,\n",
    "                    square=True,\n",
    "                    cbar=False,\n",
    "                    ax=axs[i, j],\n",
    "                )\n",
    "                axs[i, j].set_xlabel(name_dict[comparator])\n",
    "                axs[i, j].set_ylabel(name_dict[gold_standard])\n",
    "                axs[i, j].set_title(f\"{name_dict[comparator]}\")\n",
    "            print(\"\\n\")\n",
    "            sleep(0.1)\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    else:\n",
    "        # show the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics_ci(\n",
    "    gold_standards: list[str],\n",
    "    comparators: list[str],\n",
    "    dfs_concat: dict[str, pd.DataFrame] = dfs_concat,\n",
    "    name_dict: dict[str, str] = name_dict,\n",
    "    bootstrap_samples: int = 1000,\n",
    "    ci_level: float = 0.95,\n",
    "    epsilon: float = 1e-6,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"print metrics with confidence intervals\n",
    "\n",
    "    Args:\n",
    "        gold_standards (list[str]): gold standard names to evaluate against\n",
    "        comparators (list[str]): comparator names to evaluate\n",
    "        dfs_concat (dict[str, pd.DataFrame], optional): dictionary of dataframes. Defaults to dfs_concat.\n",
    "        name_dict (dict[str, str], optional): dictionary of names. Defaults to name_dict.\n",
    "        bootstrap_samples (int, optional): bootstrap samples. Defaults to 1000.\n",
    "        ci_level (float, optional): confidence interval level. Defaults to 0.95.\n",
    "        epsilon (float, optional): epsilon to avoid division by zero. Defaults to 1e-6.\n",
    "        seed (int, optional): random seed. Defaults to 42.\n",
    "    \"\"\"\n",
    "    for comparator in comparators:\n",
    "        print(name_dict[comparator])\n",
    "        print(f\"{bootstrap_samples} bootstrapped samples, {ci_level*100:.0f}% CI\")\n",
    "        for gold_standard in gold_standards:\n",
    "            print(name_dict[gold_standard])\n",
    "            # calculate metrics for the original data\n",
    "            tn, fp, fn, tp = confusion_matrix(\n",
    "                dfs_concat[gold_standard][inclusion_field],\n",
    "                dfs_concat[comparator][inclusion_field],\n",
    "            ).ravel()\n",
    "            sen = metric_dict[\"sen\"](tn, fp, fn, tp, epsilon)\n",
    "            spec = metric_dict[\"spec\"](tn, fp, fn, tp, epsilon)\n",
    "            ppv = metric_dict[\"ppv\"](tn, fp, fn, tp, epsilon)\n",
    "            npv = metric_dict[\"npv\"](tn, fp, fn, tp, epsilon)\n",
    "            plr = metric_dict[\"plr\"](tn, fp, fn, tp, epsilon)\n",
    "            nlr = metric_dict[\"nlr\"](tn, fp, fn, tp, epsilon)\n",
    "            # calculate metrics for the bootstrapped data\n",
    "            sens = []\n",
    "            specs = []\n",
    "            ppvs = []\n",
    "            npvs = []\n",
    "            plrs = []\n",
    "            nlrs = []\n",
    "            rng = np.random.default_rng(seed=seed)\n",
    "            for _ in range(bootstrap_samples):\n",
    "                bs_indices = rng.choice(\n",
    "                    range(len(dfs_concat[gold_standard])),\n",
    "                    size=len(dfs_concat[gold_standard]),\n",
    "                    replace=True,\n",
    "                )\n",
    "                tn, fp, fn, tp = confusion_matrix(\n",
    "                    dfs_concat[gold_standard][inclusion_field].iloc[bs_indices],\n",
    "                    dfs_concat[comparator][inclusion_field].iloc[bs_indices],\n",
    "                ).ravel()\n",
    "                sens.append(metric_dict[\"sen\"](tn, fp, fn, tp, epsilon))\n",
    "                specs.append(metric_dict[\"spec\"](tn, fp, fn, tp, epsilon))\n",
    "                ppvs.append(metric_dict[\"ppv\"](tn, fp, fn, tp, epsilon))\n",
    "                npvs.append(metric_dict[\"npv\"](tn, fp, fn, tp, epsilon))\n",
    "                plrs.append(metric_dict[\"plr\"](tn, fp, fn, tp, epsilon))\n",
    "                nlrs.append(metric_dict[\"nlr\"](tn, fp, fn, tp, epsilon))\n",
    "            sens = np.array(sens)\n",
    "            specs = np.array(specs)\n",
    "            ppvs = np.array(ppvs)\n",
    "            npvs = np.array(npvs)\n",
    "            plrs = np.array(plrs)\n",
    "            nlrs = np.array(nlrs)\n",
    "            sens_lower = np.percentile(sens, ((1.0 - ci_level) / 2) * 100, axis=0)\n",
    "            sens_upper = np.percentile(\n",
    "                sens, (1.0 - ((1.0 - ci_level) / 2)) * 100, axis=0\n",
    "            )\n",
    "\n",
    "            specs_lower = np.percentile(specs, ((1.0 - ci_level) / 2) * 100, axis=0)\n",
    "            specs_upper = np.percentile(\n",
    "                specs, (1.0 - ((1.0 - ci_level) / 2)) * 100, axis=0\n",
    "            )\n",
    "\n",
    "            ppvs_lower = np.percentile(ppvs, ((1.0 - ci_level) / 2) * 100, axis=0)\n",
    "            ppvs_upper = np.percentile(\n",
    "                ppvs, (1.0 - ((1.0 - ci_level) / 2)) * 100, axis=0\n",
    "            )\n",
    "\n",
    "            npvs_lower = np.percentile(npvs, ((1.0 - ci_level) / 2) * 100, axis=0)\n",
    "            npvs_upper = np.percentile(\n",
    "                npvs, (1.0 - ((1.0 - ci_level) / 2)) * 100, axis=0\n",
    "            )\n",
    "\n",
    "            plrs_lower = np.percentile(plrs, ((1.0 - ci_level) / 2) * 100, axis=0)\n",
    "            plrs_upper = np.percentile(\n",
    "                plrs, (1.0 - ((1.0 - ci_level) / 2)) * 100, axis=0\n",
    "            )\n",
    "\n",
    "            nlrs_lower = np.percentile(nlrs, ((1.0 - ci_level) / 2) * 100, axis=0)\n",
    "            nlrs_upper = np.percentile(\n",
    "                nlrs, (1.0 - ((1.0 - ci_level) / 2)) * 100, axis=0\n",
    "            )\n",
    "            # print metrics\n",
    "            print(\n",
    "                f\"Sensitivity: {sen:.2f} ({sens_lower:.2f}-{sens_upper:.2f})\".rjust(40)\n",
    "            )\n",
    "            print(\n",
    "                f\"Specificity: {spec:.2f} ({specs_lower:.2f}-{specs_upper:.2f})\".rjust(\n",
    "                    40\n",
    "                )\n",
    "            )\n",
    "            print(f\"PPV: {ppv:.2f} ({ppvs_lower:.2f}-{ppvs_upper:.2f})\".rjust(40))\n",
    "            print(f\"NPV: {npv:.2f} ({npvs_lower:.2f}-{npvs_upper:.2f})\".rjust(40))\n",
    "            print(f\"PLR: {plr:.2f} ({plrs_lower:.2f}-{plrs_upper:.2f})\".rjust(40))\n",
    "            print(f\"NLR: {nlr:.2f} ({nlrs_lower:.2f}-{nlrs_upper:.2f})\".rjust(40))\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_metrics(\n",
    "    gold_standards: list[str],\n",
    "    comparators: list[str],\n",
    "    dfs_concat: dict[str, pd.DataFrame] = dfs_concat,\n",
    "    name_dict: dict[str, str] = name_dict,\n",
    "    metrics: list[str] = [\"sen\", \"spec\", \"ppv\", \"npv\", \"plr\", \"nlr\"],\n",
    "    bootstrap_samples: int = 1000,\n",
    "    ci_level: float = 0.95,\n",
    "    plot_histograms: bool = False,\n",
    "    figsize: tuple[float, float] = (8, 5),\n",
    "    epsilon: float = 1e-6,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"compare metrics between comparators against gold standards\n",
    "\n",
    "    Args:\n",
    "        gold_standards (list[str]): gold standard names to compare against\n",
    "        comparators (list[str]): comparator names to compare against each other\n",
    "        dfs_concat (dict[str, pd.DataFrame], optional): dict of dfs. Defaults to dfs_concat.\n",
    "        name_dict (dict[str, str], optional): dict of names. Defaults to name_dict.\n",
    "        metrics (list[str], optional): list of metrics to compare. Defaults to [\"sen\", \"spec\", \"ppv\", \"npv\", \"plr\", \"nlr\"].\n",
    "        bootstrap_samples (int, optional): bootstrap samples. Defaults to 1000.\n",
    "        ci_level (float, optional): confidence interval level. Defaults to 0.95.\n",
    "        plot_histograms (bool, optional): whether to plot histograms. Defaults to False.\n",
    "        figsize (tuple[float, float], optional): figure size. Defaults to (8, 5).\n",
    "        epsilon (float, optional): epsilon to avoid division by zero. Defaults to 1e-6.\n",
    "        seed (int, optional): random seed. Defaults to 42.\n",
    "    \"\"\"\n",
    "    for gold_standard in gold_standards:\n",
    "        print(f\"Gold Standard: {name_dict[gold_standard]}\")\n",
    "        for i, comparator1 in enumerate(comparators):\n",
    "            for j, comparator2 in enumerate(comparators):\n",
    "                if j <= i:\n",
    "                    continue\n",
    "                print(\n",
    "                    f\"\\tComparing {name_dict[comparator1]} to {name_dict[comparator2]}:\"\n",
    "                )\n",
    "                # we want to compare metrics of the two comparators\n",
    "                # and report p-values for the difference\n",
    "                # using bootstrapped samples\n",
    "                for metric in metrics:\n",
    "                    # compute observed difference\n",
    "                    o_tn1, o_fp1, o_fn1, o_tp1 = confusion_matrix(\n",
    "                        dfs_concat[gold_standard][inclusion_field],\n",
    "                        dfs_concat[comparator1][inclusion_field],\n",
    "                    ).ravel()\n",
    "                    o_tn2, o_fp2, o_fn2, o_tp2 = confusion_matrix(\n",
    "                        dfs_concat[gold_standard][inclusion_field],\n",
    "                        dfs_concat[comparator2][inclusion_field],\n",
    "                    ).ravel()\n",
    "                    observed_metric1 = metric_dict[metric](\n",
    "                        o_tn1, o_fp1, o_fn1, o_tp1, epsilon\n",
    "                    )\n",
    "                    observed_metric2 = metric_dict[metric](\n",
    "                        o_tn2, o_fp2, o_fn2, o_tp2, epsilon\n",
    "                    )\n",
    "                    observed_diff = observed_metric1 - observed_metric2\n",
    "\n",
    "                    # compute bootstrapped differences\n",
    "                    rng = np.random.default_rng(seed=seed)\n",
    "                    bootstrapped_diffs = []\n",
    "                    for _ in range(bootstrap_samples):\n",
    "                        bs_indices = rng.choice(\n",
    "                            range(len(dfs_concat[gold_standard])),\n",
    "                            size=len(dfs_concat[gold_standard]),\n",
    "                            replace=True,\n",
    "                        )\n",
    "                        tn1, fp1, fn1, tp1 = confusion_matrix(\n",
    "                            dfs_concat[gold_standard][inclusion_field].iloc[bs_indices],\n",
    "                            dfs_concat[comparator1][inclusion_field].iloc[bs_indices],\n",
    "                        ).ravel()\n",
    "                        tn2, fp2, fn2, tp2 = confusion_matrix(\n",
    "                            dfs_concat[gold_standard][inclusion_field].iloc[bs_indices],\n",
    "                            dfs_concat[comparator2][inclusion_field].iloc[bs_indices],\n",
    "                        ).ravel()\n",
    "                        bs_metric1 = metric_dict[metric](tn1, fp1, fn1, tp1, epsilon)\n",
    "                        bs_metric2 = metric_dict[metric](tn2, fp2, fn2, tp2, epsilon)\n",
    "                        bootstrapped_diffs.append(bs_metric1 - bs_metric2)\n",
    "                    bootstrapped_diffs = np.array(bootstrapped_diffs)\n",
    "                    # calculate the p-value\n",
    "                    # we want to see what percentage of the bootstrapped differences cross 0\n",
    "                    p_value = np.min(\n",
    "                        [\n",
    "                            np.mean(bootstrapped_diffs > 0),\n",
    "                            np.mean(bootstrapped_diffs < 0),\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "                    # compute confidence interval\n",
    "                    lower_diff = np.percentile(\n",
    "                        bootstrapped_diffs, ((1.0 - ci_level) / 2) * 100\n",
    "                    )\n",
    "                    upper_diff = np.percentile(\n",
    "                        bootstrapped_diffs, (1.0 - ((1.0 - ci_level) / 2)) * 100\n",
    "                    )\n",
    "\n",
    "                    # if plot_histograms is True, plot the bootstrapped differences and mark the observed difference\n",
    "                    # with a vertical line at 0\n",
    "                    # shade the area of p-value\n",
    "                    if plot_histograms:\n",
    "                        plt.figure(figsize=figsize)\n",
    "                        sns.histplot(\n",
    "                            bootstrapped_diffs,\n",
    "                            kde=True,\n",
    "                            stat=\"density\",\n",
    "                            color=\"grey\",\n",
    "                            label=\"bootstrapped differences\",\n",
    "                            alpha=0.25,\n",
    "                        )\n",
    "                        plt.axvline(\n",
    "                            x=observed_diff, color=\"orange\", label=\"observed difference\"\n",
    "                        )\n",
    "                        plt.axvspan(\n",
    "                            xmin=lower_diff,\n",
    "                            xmax=upper_diff,\n",
    "                            color=\"green\",\n",
    "                            alpha=0.2,\n",
    "                            label=f\"{ci_level*100:.0f}% CI\",\n",
    "                        )\n",
    "                        if observed_diff >= 0:  # highlight values below 0\n",
    "                            plt.axvspan(\n",
    "                                xmin=-1000,\n",
    "                                xmax=0,\n",
    "                                color=\"red\",\n",
    "                                alpha=0.3,\n",
    "                                label=\"H0 true\",\n",
    "                            )\n",
    "                        else:  # highlight values above 0\n",
    "                            plt.axvspan(\n",
    "                                xmin=0,\n",
    "                                xmax=1000,\n",
    "                                color=\"red\",\n",
    "                                alpha=0.3,\n",
    "                                label=\"H0 true\",\n",
    "                            )\n",
    "                        plt.xlim(\n",
    "                            bootstrapped_diffs.min()\n",
    "                            - 0.25 * np.abs(bootstrapped_diffs.min()),\n",
    "                            bootstrapped_diffs.max()\n",
    "                            + 0.25 * np.abs(bootstrapped_diffs.max()),\n",
    "                        )\n",
    "                        plt.xlabel(\n",
    "                            f\"\\u0394 {metric_name_dict[metric]} ({name_dict[comparator1]} - {name_dict[comparator2]})\"\n",
    "                        )\n",
    "                        plt.ylabel(\"Frequency\")\n",
    "                        plt.legend()\n",
    "                        plt.show()\n",
    "\n",
    "                    print(f\"\\t\\t{metric_name_dict[metric]}:\")\n",
    "                    print(f\"\\t\\t{name_dict[comparator1]}: {observed_metric1:.2f}\")\n",
    "                    print(f\"\\t\\t{name_dict[comparator2]}: {observed_metric2:.2f}\")\n",
    "                    # greek letter delta = \\u0394\n",
    "                    print(\n",
    "                        f\"\\t\\t\\u0394: {observed_diff:.2f} [{lower_diff:.2f},{upper_diff:.2f}]\"\n",
    "                    )\n",
    "                    print(f\"\\t\\tp-value : {p_value:.3f}\")\n",
    "                    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_metrics_article(\n",
    "    gold_standards: list[str],\n",
    "    comparators: list[str],\n",
    "    dfs_concat: dict[str, pd.DataFrame] = dfs_concat,\n",
    "    name_dict: dict[str, str] = name_dict,\n",
    "    articles: list[str] = articles,\n",
    "    metrics: list[str] = [\"sen\", \"spec\", \"ppv\", \"npv\", \"plr\", \"nlr\"],\n",
    "    bootstrap_samples: int = 1000,\n",
    "    ci_level: float = 0.95,\n",
    "    plot_histograms: bool = False,\n",
    "    figsize: tuple[float, float] = (8, 5),\n",
    "    epsilon: float = 1e-6,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"Compare metrics between two comparators for each article.\n",
    "\n",
    "    Args:\n",
    "        gold_standards (list[str]): gold standard names to compare against\n",
    "        comparators (list[str]): comparator names to compare to each other\n",
    "        dfs_concat (dict[str, pd.DataFrame], optional): dictionary of dataframes. Defaults to dfs_concat.\n",
    "        name_dict (dict[str, str], optional): dictionary of names. Defaults to name_dict.\n",
    "        articles (list[str], optional): list of articles to compare. Defaults to articles.\n",
    "        metrics (list[str], optional): list of metrics to compare. Defaults to [\"sen\", \"spec\", \"ppv\", \"npv\", \"plr\", \"nlr\"].\n",
    "        bootstrap_samples (int, optional): bootstrap samples. Defaults to 1000.\n",
    "        ci_level (float, optional): confidence interval level. Defaults to 0.95.\n",
    "        plot_histograms (bool, optional): whether to plot histograms. Defaults to False.\n",
    "        figsize (tuple[float, float], optional): figure size. Defaults to (8, 5).\n",
    "        epsilon (float, optional): epsilon to avoid division by zero. Defaults to 1e-6.\n",
    "        seed (int, optional): random seed. Defaults to 42.\n",
    "    \"\"\"\n",
    "    for gold_standard in gold_standards:\n",
    "        print(f\"Gold Standard: {name_dict[gold_standard]}\")\n",
    "        for i, comparator1 in enumerate(comparators):\n",
    "            for j, comparator2 in enumerate(comparators):\n",
    "                if j <= i:\n",
    "                    continue\n",
    "                print(\n",
    "                    f\"\\tComparing {name_dict[comparator1]} to {name_dict[comparator2]}:\"\n",
    "                )\n",
    "                # we want to compare metrics of the two comparators\n",
    "                # and report p-values for the difference\n",
    "                # using bootstrapped samples\n",
    "                for article in articles:\n",
    "                    print(f\"\\t\\tArticle: {article.upper()}\")\n",
    "                    for metric in metrics:\n",
    "                        # compute observed difference\n",
    "                        o_tn1, o_fp1, o_fn1, o_tp1 = confusion_matrix(\n",
    "                            dfs_concat[gold_standard].loc[\n",
    "                                dfs_concat[gold_standard][\"article\"] == article\n",
    "                            ][inclusion_field],\n",
    "                            dfs_concat[comparator1].loc[\n",
    "                                dfs_concat[comparator1][\"article\"] == article\n",
    "                            ][inclusion_field],\n",
    "                        ).ravel()\n",
    "                        o_tn2, o_fp2, o_fn2, o_tp2 = confusion_matrix(\n",
    "                            dfs_concat[gold_standard].loc[\n",
    "                                dfs_concat[gold_standard][\"article\"] == article\n",
    "                            ][inclusion_field],\n",
    "                            dfs_concat[comparator2].loc[\n",
    "                                dfs_concat[comparator2][\"article\"] == article\n",
    "                            ][inclusion_field],\n",
    "                        ).ravel()\n",
    "                        observed_metric1 = metric_dict[metric](\n",
    "                            o_tn1, o_fp1, o_fn1, o_tp1, epsilon\n",
    "                        )\n",
    "                        observed_metric2 = metric_dict[metric](\n",
    "                            o_tn2, o_fp2, o_fn2, o_tp2, epsilon\n",
    "                        )\n",
    "                        observed_diff = observed_metric1 - observed_metric2\n",
    "\n",
    "                        # compute bootstrapped differences\n",
    "                        rng = np.random.default_rng(seed=seed)\n",
    "                        bootstrapped_diffs = []\n",
    "                        for _ in range(bootstrap_samples):\n",
    "                            bs_indices = rng.choice(\n",
    "                                range(\n",
    "                                    len(\n",
    "                                        dfs_concat[gold_standard].loc[\n",
    "                                            dfs_concat[gold_standard][\"article\"]\n",
    "                                            == article\n",
    "                                        ]\n",
    "                                    )\n",
    "                                ),\n",
    "                                size=len(\n",
    "                                    dfs_concat[gold_standard].loc[\n",
    "                                        dfs_concat[gold_standard][\"article\"] == article\n",
    "                                    ]\n",
    "                                ),\n",
    "                                replace=True,\n",
    "                            )\n",
    "                            tn1, fp1, fn1, tp1 = confusion_matrix(\n",
    "                                dfs_concat[gold_standard]\n",
    "                                .loc[dfs_concat[gold_standard][\"article\"] == article][\n",
    "                                    inclusion_field\n",
    "                                ]\n",
    "                                .iloc[bs_indices],\n",
    "                                dfs_concat[comparator1]\n",
    "                                .loc[dfs_concat[comparator1][\"article\"] == article][\n",
    "                                    inclusion_field\n",
    "                                ]\n",
    "                                .iloc[bs_indices],\n",
    "                            ).ravel()\n",
    "                            tn2, fp2, fn2, tp2 = confusion_matrix(\n",
    "                                dfs_concat[gold_standard]\n",
    "                                .loc[dfs_concat[gold_standard][\"article\"] == article][\n",
    "                                    inclusion_field\n",
    "                                ]\n",
    "                                .iloc[bs_indices],\n",
    "                                dfs_concat[comparator2]\n",
    "                                .loc[dfs_concat[comparator2][\"article\"] == article][\n",
    "                                    inclusion_field\n",
    "                                ]\n",
    "                                .iloc[bs_indices],\n",
    "                            ).ravel()\n",
    "                            bs_metric1 = metric_dict[metric](\n",
    "                                tn1, fp1, fn1, tp1, epsilon\n",
    "                            )\n",
    "                            bs_metric2 = metric_dict[metric](\n",
    "                                tn2, fp2, fn2, tp2, epsilon\n",
    "                            )\n",
    "                            bootstrapped_diffs.append(bs_metric1 - bs_metric2)\n",
    "                        bootstrapped_diffs = np.array(bootstrapped_diffs)\n",
    "                        # calculate the p-value\n",
    "                        # we want to see what percentage of the bootstrapped differences cross 0\n",
    "                        p_value = np.min(\n",
    "                            [\n",
    "                                np.mean(bootstrapped_diffs > 0),\n",
    "                                np.mean(bootstrapped_diffs < 0),\n",
    "                            ]\n",
    "                        )\n",
    "\n",
    "                        # compute confidence interval\n",
    "                        lower_diff = np.percentile(\n",
    "                            bootstrapped_diffs, ((1.0 - ci_level) / 2) * 100\n",
    "                        )\n",
    "                        upper_diff = np.percentile(\n",
    "                            bootstrapped_diffs, (1.0 - ((1.0 - ci_level) / 2)) * 100\n",
    "                        )\n",
    "\n",
    "                        # if plot_histograms is True, plot the bootstrapped differences and mark the observed difference\n",
    "                        # with a vertical line at 0\n",
    "                        # shade the area of p-value\n",
    "                        if plot_histograms:\n",
    "                            plt.figure(figsize=figsize)\n",
    "                            sns.histplot(\n",
    "                                bootstrapped_diffs,\n",
    "                                kde=True,\n",
    "                                stat=\"density\",\n",
    "                                color=\"grey\",\n",
    "                                label=\"bootstrapped differences\",\n",
    "                                alpha=0.25,\n",
    "                            )\n",
    "                            plt.axvline(\n",
    "                                x=observed_diff,\n",
    "                                color=\"orange\",\n",
    "                                label=\"observed difference\",\n",
    "                            )\n",
    "                            plt.axvspan(\n",
    "                                xmin=lower_diff,\n",
    "                                xmax=upper_diff,\n",
    "                                color=\"green\",\n",
    "                                alpha=0.2,\n",
    "                                label=f\"{ci_level*100:.0f}% CI\",\n",
    "                            )\n",
    "                            if observed_diff >= 0:  # highlight values below 0\n",
    "                                plt.axvspan(\n",
    "                                    xmin=-1000,\n",
    "                                    xmax=0,\n",
    "                                    color=\"red\",\n",
    "                                    alpha=0.3,\n",
    "                                    label=\"H0 true\",\n",
    "                                )\n",
    "                            else:  # highlight values above 0\n",
    "                                plt.axvspan(\n",
    "                                    xmin=0,\n",
    "                                    xmax=1000,\n",
    "                                    color=\"red\",\n",
    "                                    alpha=0.3,\n",
    "                                    label=\"H0 true\",\n",
    "                                )\n",
    "                            plt.xlim(\n",
    "                                bootstrapped_diffs.min()\n",
    "                                - 0.25 * np.abs(bootstrapped_diffs.min()),\n",
    "                                bootstrapped_diffs.max()\n",
    "                                + 0.25 * np.abs(bootstrapped_diffs.max()),\n",
    "                            )\n",
    "                            plt.xlabel(\n",
    "                                f\"\\u0394 {metric_name_dict[metric]} ({name_dict[comparator1]} - {name_dict[comparator2]}) in {article.upper()}\"\n",
    "                            )\n",
    "                            plt.ylabel(\"Frequency\")\n",
    "                            plt.legend()\n",
    "                            plt.show()\n",
    "\n",
    "                        print(f\"\\t\\t{metric_name_dict[metric]}:\")\n",
    "                        print(f\"\\t\\t{name_dict[comparator1]}: {observed_metric1:.2f}\")\n",
    "                        print(f\"\\t\\t{name_dict[comparator2]}: {observed_metric2:.2f}\")\n",
    "                        # greek letter delta = \\u0394\n",
    "                        print(\n",
    "                            f\"\\t\\t\\u0394: {observed_diff:.2f} [{lower_diff:.2f},{upper_diff:.2f}]\"\n",
    "                        )\n",
    "                        print(f\"\\t\\tp-value: {p_value:.3f}\")\n",
    "                        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_article(\n",
    "    gold_standards: list[str],\n",
    "    comparators: list[str],\n",
    "    articles: list[str] = articles,\n",
    "    name_dict: dict[str, str] = name_dict,\n",
    "    dfs_concat: dict[str, pd.DataFrame] = dfs_concat,\n",
    "    size: float = 5,\n",
    "    save_path: str = None,\n",
    "):\n",
    "    \"\"\"plots confusion matrices for each article\n",
    "\n",
    "    Args:\n",
    "        gold_standards (list[str]): gold standard names to compare against\n",
    "        comparators (list[str]): comparator names\n",
    "        articles (list[str], optional): articles to plot. Defaults to articles.\n",
    "        name_dict (dict[str, str], optional): dictionary of names. Defaults to name_dict.\n",
    "        dfs_concat (dict[str, pd.DataFrame], optional): dictionary of dataframes. Defaults to dfs_concat.\n",
    "        size (float, optional): size of plot. Defaults to 5.\n",
    "    \"\"\"\n",
    "    for article in articles:\n",
    "        print(f\"Article: {article.upper()}\")\n",
    "        fig, axs = plt.subplots(\n",
    "            len(comparators),\n",
    "            len(gold_standards),\n",
    "            sharex=True,\n",
    "            sharey=True,\n",
    "            figsize=(len(comparators) * size, len(gold_standards) * size),\n",
    "        )\n",
    "        for i, comparator in enumerate(comparators):\n",
    "            print(f\"{name_dict[comparator]}\")\n",
    "            for j, gold_standard in enumerate(gold_standards):\n",
    "                print(f\"\\t{name_dict[gold_standard]}\")\n",
    "                cm = confusion_matrix(\n",
    "                    dfs_concat[gold_standard].loc[\n",
    "                        dfs_concat[gold_standard][\"article\"] == article\n",
    "                    ][inclusion_field],\n",
    "                    dfs_concat[comparator].loc[\n",
    "                        dfs_concat[comparator][\"article\"] == article\n",
    "                    ][inclusion_field],\n",
    "                )\n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "                npv = tn / (tn + fn)\n",
    "                ppv = tp / (tp + fp)\n",
    "                sen = tp / (tp + fn)\n",
    "                spec = tn / (tn + fp)\n",
    "                plr = sen / (1 - spec)\n",
    "                nlr = (1 - sen) / spec\n",
    "                print(f\"tn: {tn}, fp: {fp}, fn: {fn}, tp: {tp}\")\n",
    "                print(\n",
    "                    f\"NPV: {npv:.2f}, PPV: {ppv:.2f}, \\nSensitivity: {sen:.2f}, Specificity: {spec:.2f}\\nPLR: {plr:.2f}, NLR: {nlr:.2f}\"\n",
    "                )\n",
    "                # normalize the confusion matrix\n",
    "                cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "                if len(gold_standards) == len(comparators) == 1:\n",
    "                    sns.heatmap(\n",
    "                        cm,\n",
    "                        annot=True,\n",
    "                        xticklabels=[\"Excluded\", \"Included\"],\n",
    "                        yticklabels=[\"Excluded\", \"Included\"],\n",
    "                        cmap=\"Blues\",\n",
    "                        vmin=0,\n",
    "                        vmax=1,\n",
    "                        cbar=False,\n",
    "                        square=True,\n",
    "                        ax=axs,\n",
    "                    )\n",
    "                    axs.set_xlabel(f\"{name_dict[comparator]}\")\n",
    "                    axs.set_ylabel(name_dict[gold_standard])\n",
    "                    axs.set_title(\n",
    "                        f\"{article.upper()}: {name_dict[comparator]}\",\n",
    "                        fontsize=7.5,\n",
    "                    )\n",
    "                elif (\n",
    "                    len(gold_standards) == 1 or len(comparators) == 1\n",
    "                ):  # only one gold standard or comparator\n",
    "                    sns.heatmap(\n",
    "                        cm,\n",
    "                        annot=True,\n",
    "                        xticklabels=[\"Excluded\", \"Included\"],\n",
    "                        yticklabels=[\"Excluded\", \"Included\"],\n",
    "                        cmap=\"Blues\",\n",
    "                        vmin=0,\n",
    "                        vmax=1,\n",
    "                        cbar=False,\n",
    "                        square=True,\n",
    "                        ax=axs[i + j],\n",
    "                    )\n",
    "                    axs[i + j].set_xlabel(f\"{name_dict[comparator]}\")\n",
    "                    axs[i + j].set_ylabel(name_dict[gold_standard])\n",
    "                    axs[i + j].set_title(\n",
    "                        f\"{article.upper()}: {name_dict[comparator]}\",\n",
    "                        fontsize=7.5,\n",
    "                    )\n",
    "                else:\n",
    "                    sns.heatmap(\n",
    "                        cm,\n",
    "                        annot=True,\n",
    "                        xticklabels=[\"Excluded\", \"Included\"],\n",
    "                        yticklabels=[\"Excluded\", \"Included\"],\n",
    "                        cmap=\"Blues\",\n",
    "                        vmin=0,\n",
    "                        vmax=1,\n",
    "                        cbar=False,\n",
    "                        square=True,\n",
    "                        ax=axs[i, j],\n",
    "                    )\n",
    "                    axs[i, j].set_xlabel(f\"{name_dict[comparator]}\")\n",
    "                    axs[i, j].set_ylabel(name_dict[gold_standard])\n",
    "                    axs[i, j].set_title(\n",
    "                        f\"{article.upper()}: {name_dict[comparator]}\",\n",
    "                        fontsize=7.5,\n",
    "                    )\n",
    "                sleep(0.1)\n",
    "        if save_path is not None:\n",
    "            plt.savefig(save_path, dpi=300)\n",
    "        else: \n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_c_index(\n",
    "    gold_standards: list[str],\n",
    "    comparators: list[str],\n",
    "    articles: list[str] = articles,\n",
    "    name_dict: dict[str, str] = name_dict,\n",
    "    dfs_concat: dict[str, pd.DataFrame] = dfs_concat,\n",
    "):\n",
    "    \"\"\"prints the c-index for each article\"\"\"\n",
    "    print(\"C-index \")\n",
    "    for comparator in comparators:\n",
    "        print(f\"\\n{name_dict[comparator]}\")\n",
    "        for gold_standard in gold_standards:\n",
    "            print(f\"\\t{name_dict[gold_standard]}\")\n",
    "            for article in articles:\n",
    "                print(\n",
    "                    f\"\"\"{article.upper()}: {concordance_index(dfs_concat[gold_standard].loc[dfs_concat[gold_standard]['article'] == article][inclusion_field], \n",
    "                    dfs_concat[comparator].loc[dfs_concat[comparator]['article'] == article][inclusion_original_field]):.2f}\"\"\".rjust(\n",
    "                        20\n",
    "                    )\n",
    "                )\n",
    "            # print the total c-index\n",
    "            print(\n",
    "                f\"\"\"Total: {concordance_index(dfs_concat[gold_standard][inclusion_field], dfs_concat[comparator][inclusion_original_field]):.2f}\"\"\".rjust(\n",
    "                    20\n",
    "                ),\n",
    "                end=\"\\n\\n\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_jaccard(\n",
    "    gold_standards: list[str],\n",
    "    comparators: list[str],\n",
    "    articles: list[str] = articles,\n",
    "    name_dict: dict[str, str] = name_dict,\n",
    "    dfs_concat: dict[str, pd.DataFrame] = dfs_concat,\n",
    "    ci_level: float = 0.95,\n",
    "    n_bootstraps: int = 1000,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"prints the jaccard index for each article with confidence intervals\"\"\"\n",
    "    print(\"Jaccard index\")\n",
    "    for comparator in comparators:\n",
    "        print(f\"\\n{name_dict[comparator]}\")\n",
    "        for gold_standard in gold_standards:\n",
    "            print(f\"\\t{name_dict[gold_standard]}\")\n",
    "            for article in articles:\n",
    "                rng = np.random.default_rng(seed)\n",
    "                bs_jaccard_scores = []\n",
    "                for _ in range(n_bootstraps):\n",
    "                    bs_indices = rng.choice(\n",
    "                        range(\n",
    "                            len(\n",
    "                                dfs_concat[gold_standard].loc[\n",
    "                                    dfs_concat[gold_standard][\"article\"] == article\n",
    "                                ][inclusion_field]\n",
    "                            )\n",
    "                        ),\n",
    "                        size=len(\n",
    "                            dfs_concat[gold_standard].loc[\n",
    "                                dfs_concat[gold_standard][\"article\"] == article\n",
    "                            ][inclusion_field]\n",
    "                        ),\n",
    "                        replace=True,\n",
    "                    )\n",
    "                    bs_jaccard_scores.append(\n",
    "                        jaccard_score(\n",
    "                            dfs_concat[gold_standard]\n",
    "                            .loc[dfs_concat[gold_standard][\"article\"] == article][\n",
    "                                inclusion_field\n",
    "                            ]\n",
    "                            .iloc[bs_indices],\n",
    "                            dfs_concat[comparator]\n",
    "                            .loc[dfs_concat[comparator][\"article\"] == article][\n",
    "                                inclusion_field\n",
    "                            ]\n",
    "                            .iloc[bs_indices],\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                bs_jaccard_scores = np.array(bs_jaccard_scores)\n",
    "                upper_bound = np.quantile(\n",
    "                    bs_jaccard_scores,\n",
    "                    (1 + ci_level) / 2,\n",
    "                )\n",
    "                lower_bound = np.quantile(\n",
    "                    bs_jaccard_scores,\n",
    "                    (1 - ci_level) / 2,\n",
    "                )\n",
    "                observed_jaccard = jaccard_score(\n",
    "                    dfs_concat[gold_standard].loc[\n",
    "                        dfs_concat[gold_standard][\"article\"] == article\n",
    "                    ][inclusion_field],\n",
    "                    dfs_concat[comparator].loc[\n",
    "                        dfs_concat[comparator][\"article\"] == article\n",
    "                    ][inclusion_field],\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"\"\"{article.upper()}: {observed_jaccard:.2f} [{lower_bound:.2f}, {upper_bound:.2f}]\"\"\".rjust(\n",
    "                        20\n",
    "                    )\n",
    "                )\n",
    "            # print the total jaccard index\n",
    "            rng = np.random.default_rng(seed)\n",
    "            bs_jaccard_scores = []\n",
    "            for _ in range(n_bootstraps):\n",
    "                bs_indices = rng.choice(\n",
    "                    range(len(dfs_concat[gold_standard][inclusion_field])),\n",
    "                    size=len(dfs_concat[gold_standard][inclusion_field]),\n",
    "                    replace=True,\n",
    "                )\n",
    "                bs_jaccard_scores.append(\n",
    "                    jaccard_score(\n",
    "                        dfs_concat[gold_standard][inclusion_field].iloc[bs_indices],\n",
    "                        dfs_concat[comparator][inclusion_field].iloc[bs_indices],\n",
    "                    )\n",
    "                )\n",
    "            bs_jaccard_scores = np.array(bs_jaccard_scores)\n",
    "            upper_bound = np.quantile(\n",
    "                bs_jaccard_scores,\n",
    "                (1 + ci_level) / 2,\n",
    "            )\n",
    "            lower_bound = np.quantile(\n",
    "                bs_jaccard_scores,\n",
    "                (1 - ci_level) / 2,\n",
    "            )\n",
    "            observed_jaccard = jaccard_score(\n",
    "                dfs_concat[gold_standard][inclusion_field],\n",
    "                dfs_concat[comparator][inclusion_field],\n",
    "            )\n",
    "            print(\n",
    "                f\"Total: {observed_jaccard:.2f} [{lower_bound:.2f}, {upper_bound:.2f}]\".rjust(\n",
    "                    20\n",
    "                ),\n",
    "                end=\"\\n\\n\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_balanced_accuracy(\n",
    "    gold_standards: list[str],\n",
    "    comparators: list[str],\n",
    "    articles: list[str] = articles,\n",
    "    name_dict: dict[str, str] = name_dict,\n",
    "    dfs_concat: dict[str, pd.DataFrame] = dfs_concat,\n",
    "):\n",
    "    \"\"\"prints the balanced accuracy score for each article\"\"\"\n",
    "    print(\"Balanced Accuracy Score\")\n",
    "    for comparator in comparators:\n",
    "        print(f\"\\n{name_dict[comparator]}\")\n",
    "        for gold_standard in gold_standards:\n",
    "            print(f\"\\t{name_dict[gold_standard]}\")\n",
    "            for article in articles:\n",
    "                print(\n",
    "                    f\"\"\"{article.upper()}: {balanced_accuracy_score(dfs_concat[gold_standard].loc[dfs_concat[gold_standard]['article'] == article][inclusion_field], \n",
    "                        dfs_concat[comparator].loc[dfs_concat[comparator]['article'] == article][inclusion_field]):.2f}\"\"\".rjust(\n",
    "                        20\n",
    "                    )\n",
    "                )\n",
    "            # print the total accuracy score\n",
    "            print(\n",
    "                f\"\"\"Total: {balanced_accuracy_score(dfs_concat[gold_standard][inclusion_field], dfs_concat[comparator][inclusion_field]):.2f}\"\"\".rjust(\n",
    "                    20\n",
    "                ),\n",
    "                end=\"\\n\\n\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_balanced_accuracy_ci(\n",
    "    gold_standards: list[str],\n",
    "    comparators: list[str],\n",
    "    articles: list[str] = articles,\n",
    "    name_dict: dict[str, str] = name_dict,\n",
    "    dfs_concat: dict[str, pd.DataFrame] = dfs_concat,\n",
    "    ci_level: float = 0.95,\n",
    "    n_bootstraps: int = 1000,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"prints the balanced accuracy score for each article with confidence intervals\n",
    "\n",
    "    Args:\n",
    "        gold_standards (list[str]): gold standard names\n",
    "        comparators (list[str]): comparator names\n",
    "        articles (list[str], optional): article names. Defaults to articles.\n",
    "        name_dict (dict[str, str], optional): name dictionary. Defaults to name_dict.\n",
    "        dfs_concat (dict[str, pd.DataFrame], optional): dictionary of dataframes. Defaults to dfs_concat.\n",
    "        ci_level (float, optional): confidence interval level. Defaults to 0.95.\n",
    "        n_bootstraps (int, optional): number of bootstraps. Defaults to 1000.\n",
    "        seed (int, optional): random seed. Defaults to 42.\n",
    "    \"\"\"\n",
    "    print(\"Balanced Accuracy Score\")\n",
    "    for comparator in comparators:\n",
    "        print(f\"\\n{name_dict[comparator]}\")\n",
    "        for gold_standard in gold_standards:\n",
    "            print(f\"\\t{name_dict[gold_standard]}\")\n",
    "            for article in articles:\n",
    "                rng = np.random.default_rng(seed)\n",
    "                bs_accuracy_scores = []\n",
    "                for _ in range(n_bootstraps):\n",
    "                    bs_indices = rng.choice(\n",
    "                        range(\n",
    "                            len(\n",
    "                                dfs_concat[gold_standard].loc[\n",
    "                                    dfs_concat[gold_standard][\"article\"] == article\n",
    "                                ][inclusion_field]\n",
    "                            )\n",
    "                        ),\n",
    "                        size=len(\n",
    "                            dfs_concat[gold_standard].loc[\n",
    "                                dfs_concat[gold_standard][\"article\"] == article\n",
    "                            ][inclusion_field]\n",
    "                        ),\n",
    "                        replace=True,\n",
    "                    )\n",
    "                    bs_accuracy_scores.append(\n",
    "                        balanced_accuracy_score(\n",
    "                            dfs_concat[gold_standard]\n",
    "                            .loc[dfs_concat[gold_standard][\"article\"] == article][\n",
    "                                inclusion_field\n",
    "                            ]\n",
    "                            .iloc[bs_indices],\n",
    "                            dfs_concat[comparator]\n",
    "                            .loc[dfs_concat[comparator][\"article\"] == article][\n",
    "                                inclusion_field\n",
    "                            ]\n",
    "                            .iloc[bs_indices],\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                bs_accuracy_scores = np.array(bs_accuracy_scores)\n",
    "                upper_bound = np.quantile(\n",
    "                    bs_accuracy_scores,\n",
    "                    (1 + ci_level) / 2,\n",
    "                )\n",
    "                lower_bound = np.quantile(\n",
    "                    bs_accuracy_scores,\n",
    "                    (1 - ci_level) / 2,\n",
    "                )\n",
    "                observed_accuracy_score = balanced_accuracy_score(\n",
    "                    dfs_concat[gold_standard].loc[\n",
    "                        dfs_concat[gold_standard][\"article\"] == article\n",
    "                    ][inclusion_field],\n",
    "                    dfs_concat[comparator].loc[\n",
    "                        dfs_concat[comparator][\"article\"] == article\n",
    "                    ][inclusion_field],\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"{article.upper()}: {observed_accuracy_score:.2f} [{lower_bound:.2f}, {upper_bound:.2f}]\".rjust(\n",
    "                        20\n",
    "                    )\n",
    "                )\n",
    "            # print the total accuracy score\n",
    "            rng = np.random.default_rng(seed)\n",
    "            bs_accuracy_scores = []\n",
    "            for _ in range(n_bootstraps):\n",
    "                bs_indices = rng.choice(\n",
    "                    range(len(dfs_concat[gold_standard][inclusion_field])),\n",
    "                    size=len(dfs_concat[gold_standard][inclusion_field]),\n",
    "                    replace=True,\n",
    "                )\n",
    "                bs_accuracy_scores.append(\n",
    "                    balanced_accuracy_score(\n",
    "                        dfs_concat[gold_standard][inclusion_field].iloc[bs_indices],\n",
    "                        dfs_concat[comparator][inclusion_field].iloc[bs_indices],\n",
    "                    )\n",
    "                )\n",
    "            bs_accuracy_scores = np.array(bs_accuracy_scores)\n",
    "            upper_bound = np.quantile(\n",
    "                bs_accuracy_scores,\n",
    "                (1 + ci_level) / 2,\n",
    "            )\n",
    "            lower_bound = np.quantile(\n",
    "                bs_accuracy_scores,\n",
    "                (1 - ci_level) / 2,\n",
    "            )\n",
    "            observed_accuracy_score = balanced_accuracy_score(\n",
    "                dfs_concat[gold_standard][inclusion_field],\n",
    "                dfs_concat[comparator][inclusion_field],\n",
    "            )\n",
    "            print(\n",
    "                f\"Total: {observed_accuracy_score:.2f} [{lower_bound:.2f}, {upper_bound:.2f}]\".rjust(\n",
    "                    20\n",
    "                ),\n",
    "                end=\"\\n\\n\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_kappa_ci(human_raters + gpt_raters, name_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kappas_article(articles, human_raters + gpt_raters, size=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kappas(raters, figsize=(6.5,6.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare gpt-scores to consensus by experts\n",
    "plot_roc_ci(\n",
    "    gold_standards=consensus_types,\n",
    "    comparators=[\"gpt\"],\n",
    "    size=12,\n",
    "    ci_level=0.95,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare gpt-scores to consensus by experts\n",
    "plot_confusion(\n",
    "    consensus_types,\n",
    "    gpt_raters+gp_raters+consensus_types_gp,\n",
    "    size=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics_ci(\n",
    "    [\"vote\"],\n",
    "    [\"gpt\"] + gp_raters + consensus_types_gp,\n",
    "    name_dict,\n",
    "    dfs_concat,\n",
    "    ci_level=0.95,\n",
    "    bootstrap_samples=1000,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_metrics(\n",
    "    [\"vote\"],\n",
    "    [\"gpt\", \"vote_gp\", \"sens_con_gp\"],\n",
    "    metrics=[\"sen\", \"spec\", \"ppv\", \"npv\"],\n",
    "    name_dict=name_dict,\n",
    "    bootstrap_samples=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_metrics_article(\n",
    "    [\"vote\"],\n",
    "    [\"gpt\", \"sens_con_gp\"],\n",
    "    name_dict,\n",
    "    dfs_concat,\n",
    "    [\"colorectal\"],\n",
    "    [\"sen\", \"spec\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare gpt-scores to consensus by experts - article level\n",
    "for article in articles:\n",
    "    plot_confusion_article( \n",
    "        gold_standards=consensus_types,\n",
    "        comparators=gpt_raters+gp_raters+consensus_types_gp,\n",
    "        articles=[article],\n",
    "        size=8,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_c_index(\n",
    "    [\"vote\", \"sens_con\"], gpt_raters, name_dict, name_dict, articles, dfs_concat\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_jaccard(\n",
    "    gold_standards=[\"vote\", \"sens_con\"],\n",
    "    comparators=gp_raters + gpt_raters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_balanced_accuracy(\n",
    "    [\"vote\", \"sens_con\"],\n",
    "    gp_raters + gpt_raters,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_balanced_accuracy_ci([\"vote\"], gp_raters + consensus_types_gp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_jaccard([\"vote\"], gp_raters + consensus_types_gp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_inclusions = 0\n",
    "for rater in human_raters:\n",
    "    print(\n",
    "        f\"{name_dict[rater]} included {dfs_concat[rater][inclusion_field].sum()} articles out of {len(dfs_concat[rater])} ({dfs_concat[rater][inclusion_field].sum()/len(dfs_concat[rater]):.2%})\"\n",
    "    )\n",
    "    total_inclusions += dfs_concat[rater][inclusion_field].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rater in raters:\n",
    "    print(f\"{name_dict[rater]}\")\n",
    "    for article in articles:\n",
    "        print(\n",
    "            f\"{article.upper()}: {dfs_concat[rater].loc[dfs_concat[rater]['article'] == article][inclusion_field].sum()} out of {len(dfs_concat[rater].loc[dfs_concat[rater]['article'] == article])} ({dfs_concat[rater].loc[dfs_concat[rater]['article'] == article][inclusion_field].sum()/len(dfs_concat[rater].loc[dfs_concat[rater]['article'] == article]):.2%})\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
